{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # number of independent sequences will we process in parallel\n",
        "block_size = 32 # maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJb_ReIw3kG7",
        "outputId": "07df1ec2-963b-44f1-8138-9ff67ec93127"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc0b6fe68b0>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n"
      ],
      "metadata": {
        "id": "Z2TMIcL81gYS"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"the dataset has this many characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ednxSocD2PHY",
        "outputId": "1ed2df03-cb5f-42e7-bee6-c745e61b1356"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the dataset has this many characters:  484819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"first 1000 characters of dataset: \", text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJhsY1od2eQx",
        "outputId": "d8f911f6-e681-4e58-da8d-433c2255e412"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first 1000 characters of dataset:  ﻿A JOURNEY TO THE CENTRE OF THE EARTH\n",
            "By Jules Verne\n",
            "\n",
            "\n",
            "CHAPTER 1\n",
            "\n",
            "MY UNCLE MAKES A GREAT DISCOVERY\n",
            "\n",
            "\n",
            "Looking back to all that has occurred to me since that eventful day, I\n",
            "am scarcely able to believe in the reality of my adventures. They were\n",
            "truly so wonderful that even now I am bewildered when I think of them.\n",
            "\n",
            "My uncle was a German, having married my mother's sister, an\n",
            "Englishwoman. Being very much attached to his fatherless nephew, he\n",
            "invited me to study under him in his home in the fatherland. This home\n",
            "was in a large town, and my uncle a professor of philosophy, chemistry,\n",
            "geology, mineralogy, and many other ologies.\n",
            "\n",
            "One day, after passing some hours in the laboratory--my uncle being\n",
            "absent at the time--I suddenly felt the necessity of renovating the\n",
            "tissues--<i>i.e.</i>, I was hungry, and was about to rouse up our old French\n",
            "cook, when my uncle, Professor Von Hardwigg, suddenly opened the street\n",
            "door, and came rushing upstairs.\n",
            "\n",
            "Now Professor Hardwigg, my worthy uncle, is by n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list of all unique characters in the dataset\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKcpbuiO2_7R",
        "outputId": "b4538d80-ff17-49da-c35a-ef60451efc9d"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"'()*+,-./0123456789:;<>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz£﻿\n",
            "84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: change the string input into list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: change the list of integers and output a string\n"
      ],
      "metadata": {
        "id": "T3XwYvub393K"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "Cejz7uqi4oN7"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    # this function generates a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "pwWUlFu64sUe"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "_1mYoL-b40-i"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    #self attention\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Kip7UNmQ46YC"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    # multiple heads of self-attention in parallel\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "DTSiixNX55sr"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "pJyYjMAp6FbV"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "T6N0lEK86MOP"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self): # each token directly reads off the logits for the next token from a lookup table\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "JzqDGHtz7aKb"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(\"number of parameters in the model\", sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\") # show loss in intervals in train and val sets\n",
        "\n",
        "    xb, yb = get_batch('train') # sample batch\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6f1Yqgf5wrX",
        "outputId": "128670bd-ffea-47e8-bddf-f1479c2f41a6"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters in the model 0.21218 M parameters\n",
            "step 0: train loss 4.5747, val loss 4.5699\n",
            "step 100: train loss 2.6146, val loss 2.6150\n",
            "step 200: train loss 2.4942, val loss 2.4927\n",
            "step 300: train loss 2.4178, val loss 2.4111\n",
            "step 400: train loss 2.3581, val loss 2.3473\n",
            "step 500: train loss 2.2893, val loss 2.2669\n",
            "step 600: train loss 2.2197, val loss 2.2093\n",
            "step 700: train loss 2.1691, val loss 2.1597\n",
            "step 800: train loss 2.1244, val loss 2.1202\n",
            "step 900: train loss 2.0936, val loss 2.0818\n",
            "step 1000: train loss 2.0568, val loss 2.0623\n",
            "step 1100: train loss 2.0076, val loss 2.0074\n",
            "step 1200: train loss 1.9973, val loss 1.9905\n",
            "step 1300: train loss 1.9634, val loss 1.9654\n",
            "step 1400: train loss 1.9355, val loss 1.9368\n",
            "step 1500: train loss 1.9175, val loss 1.9284\n",
            "step 1600: train loss 1.8878, val loss 1.8926\n",
            "step 1700: train loss 1.8718, val loss 1.8835\n",
            "step 1800: train loss 1.8559, val loss 1.8618\n",
            "step 1900: train loss 1.8359, val loss 1.8522\n",
            "step 2000: train loss 1.8165, val loss 1.8245\n",
            "step 2100: train loss 1.8159, val loss 1.8235\n",
            "step 2200: train loss 1.7977, val loss 1.8156\n",
            "step 2300: train loss 1.7832, val loss 1.8029\n",
            "step 2400: train loss 1.7806, val loss 1.7874\n",
            "step 2500: train loss 1.7466, val loss 1.7735\n",
            "step 2600: train loss 1.7501, val loss 1.7782\n",
            "step 2700: train loss 1.7402, val loss 1.7610\n",
            "step 2800: train loss 1.7215, val loss 1.7531\n",
            "step 2900: train loss 1.7118, val loss 1.7349\n",
            "step 3000: train loss 1.7042, val loss 1.7177\n",
            "step 3100: train loss 1.6909, val loss 1.7194\n",
            "step 3200: train loss 1.6893, val loss 1.7010\n",
            "step 3300: train loss 1.6806, val loss 1.7026\n",
            "step 3400: train loss 1.6708, val loss 1.7022\n",
            "step 3500: train loss 1.6563, val loss 1.6765\n",
            "step 3600: train loss 1.6598, val loss 1.6815\n",
            "step 3700: train loss 1.6434, val loss 1.6833\n",
            "step 3800: train loss 1.6393, val loss 1.6701\n",
            "step 3900: train loss 1.6226, val loss 1.6574\n",
            "step 4000: train loss 1.6252, val loss 1.6634\n",
            "step 4100: train loss 1.6263, val loss 1.6615\n",
            "step 4200: train loss 1.6099, val loss 1.6477\n",
            "step 4300: train loss 1.6103, val loss 1.6453\n",
            "step 4400: train loss 1.6110, val loss 1.6431\n",
            "step 4500: train loss 1.5895, val loss 1.6408\n",
            "step 4600: train loss 1.5930, val loss 1.6306\n",
            "step 4700: train loss 1.5867, val loss 1.6369\n",
            "step 4800: train loss 1.5768, val loss 1.6226\n",
            "step 4900: train loss 1.5812, val loss 1.6178\n",
            "step 4999: train loss 1.5729, val loss 1.6252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate using the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsRZwetK_inX",
        "outputId": "e5e89712-5cac-4ed1-8a43-b248dcf50c0a"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The sucte of our mazzes ready all. We called, and to gaum; amite\n",
            "desfarting teauses however down the instantly open.\n",
            "\n",
            "At The spass be romeck fallords.\n",
            "\n",
            "\"We clasted,\" As situate thato the roce.\n",
            "\n",
            "As long to bottonom this rong of slarm there last--works be mutgight, we mut with we degressarls, and thus light end Capinht\n",
            "dissible to bott the saphute, hus posses and comparing forth.\n",
            "\n",
            "The confical\n",
            "any namiral tow that I epens all knictic lass, surpened--quiestant narm upon almose possessed to sking, my no hand, with upone sucumY cleard. Our tone, the excens\n",
            "any carrecting pets of tenind\n",
            "posituries my anwell of enthaps simprished\n",
            "thertem of thinks and was can explicume dorgeted us,\n",
            "camporded of the live\n",
            "rumbarcaut of reply humand verknows--lomety have nume sefful ordsarrned tid to bacle mans to speep light sobility solonds othalosonisal in vas a profie.\n",
            "\n",
            "Scullend my spaceun to come\n",
            "freption over stanet prucious\n",
            "conlone.\n",
            "\n",
            "\"We spectance, and yet forther volculater from\n",
            "monorning his were\n",
            "auch ropply a surface of suggres up\n",
            "fissparth.\"\n",
            "\n",
            "Harry incland\n",
            "recees of My renepine, them and time--litten, I begn he breaths, and wenth muddent\n",
            "rocks, thus and in a plene\n",
            "languan towast when to ere on unally ordinart\n",
            "press libruct gones invited upon surried by any thind!\n",
            "\n",
            "We was Suddenly; a noldi as on a mumer, and takems. Surrate.\n",
            "\n",
            "A fill projecticate of up--you to hum enthusiastful spoke as discase, guip thaps it\n",
            "time, sudge\n",
            "as sleep as bottle which a dreamed of reasonnes Carted timply aire the which terrible his sudden.\n",
            "\n",
            "\"Cinuring chans. Gexturume to trilide. We\n",
            "somply satual be mast on the\n",
            "most that where, I alrid, anvetion\n",
            "anxious cyrtanes of my was pices nakly porpude alway was to cure and Icelands fone sadinute of tun I'clouth'sly in\n",
            "state.\n",
            "\n",
            "Herthe soloniasson.\n",
            "\n",
            "\"What ruld it to my suckness in this attance sudde, and to take miny\n",
            "sensemmenting.\n",
            "This vion were: han studes, allowiye\n",
            "rallines!\"\n",
            "\n",
            "\"What veginom like obaned to firtune.\n",
            "\n",
            "\"What,\" sail, was \"if Compass perishlaps in over dir\n"
          ]
        }
      ]
    }
  ]
}